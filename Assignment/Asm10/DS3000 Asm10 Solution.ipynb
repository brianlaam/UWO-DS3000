{"cells":[{"cell_type":"markdown","metadata":{"id":"ggO9ePRsm1gH"},"source":["\n","# Assignment 10: Prompt Engineering with LLaMA 3\n","\n","### Objective\n","In this assignment, you will explore the use of Large Language Models (LLMs)—specifically the Meta LLaMA 3.2B Instruct model—for the task of sentiment classification. The model will classify movie reviews as either positive or negative and provide an explanation for its decision. You will then explore how different prompt engineering strategies affect the model's behavior and explanations.\n","\n","### What You’ll Learn\n","- How to prompt an LLM for classification tasks.\n","- How to interpret and evaluate LLM explanations.\n","- The effect of prompt design on prediction quality and reliability.\n","\n","### Tools\n","- Dataset: [IMDb Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)\n","- Model: `meta-llama/Llama-3.2-3B-Instruct` from HuggingFace\n","- Libraries: `transformers`, `datasets`, `torch`"],"id":"ggO9ePRsm1gH"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6RqtRXgem1gJ"},"outputs":[],"source":["# Install dependencies\n","!pip install -q transformers datasets accelerate bitsandbytes"],"id":"6RqtRXgem1gJ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhWoSy2cm1gK"},"outputs":[],"source":["# Login to Hugging Face to access gated models like LLaMA. You can also set a secret in Colab with you token.\n","from huggingface_hub import login\n","\n","# This will prompt you to paste your token securely\n","login()"],"id":"NhWoSy2cm1gK"},{"cell_type":"code","execution_count":null,"metadata":{"id":"n77_ZEE4m1gL"},"outputs":[],"source":["# Import necessary libraries\n","import re\n","import torch\n","import random\n","import polars as pl\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"],"id":"n77_ZEE4m1gL"},{"cell_type":"markdown","metadata":{"id":"gLcUfPiam1gL"},"source":["### Question 1: Load and Explore the IMDb Dataset\n","\n","You'll be using the IMDb dataset, which contains 50,000 movie reviews labeled as positive or negative.\n","1. Use the [`datasets`](https://huggingface.co/docs/datasets/create_dataset) library to load the IMDb dataset. Read the documentation to understand how to do this.\n","2. Extract the `train` and `test` splits.\n","3. Print:\n","   - The number of training and test samples.\n","   - Three random training samples with both the text and their sentiment labels."],"id":"gLcUfPiam1gL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRP61YRPm1gL"},"outputs":[],"source":["# Load the dataset\n","\n","\n","\n","\n","\n","\n","\n","# Show three random examples\n","\n","\n","\n","\n","\n"],"id":"KRP61YRPm1gL"},{"cell_type":"markdown","metadata":{"id":"U9AEMFu7m1gL"},"source":["\n","### Question 2: Define a Prompting Function for LLaMA\n","\n","You'll write a function that sends a movie review to the LLaMA model and returns a sentiment classification along with an explanation.\n","1. Use the `meta-llama/Llama-3.2-3B-Instruct` model from HuggingFace.\n","2. Format the prompt like this:\n","   ```\n","   You are a helpful AI assistant.\n","   Given the following movie review, classify it as Positive or Negative and explain why.\n","\n","   Review: \"<REVIEW TEXT>\"\n","\n","   Sentiment:\n","   ```\n","3. Use HuggingFace’s `generate()` function with decoding parameters like:\n","   - `temperature = 0.7`\n","   - `top_p = 0.95`\n","   - `max_new_tokens = 200`\n","4. Make sure to decode the result and return the LLM’s classification and explanation."],"id":"U9AEMFu7m1gL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ak0O0xsim1gM"},"outputs":[],"source":["# Load tokenizer and model\n","\n","\n","\n","\n","\n","\n","\n","# Function to get prediction from LLM\n","\n","\n","\n","\n","\n"],"id":"Ak0O0xsim1gM"},{"cell_type":"markdown","metadata":{"id":"52JhMeQ1m1gM"},"source":["\n","### Question 3: Run the LLaMA Model on Real Reviews\n","\n","Now that your model function is ready, apply it to real data.\n","1. Randomly select 50 test reviews from the IMDb dataset.\n","2. For each review:\n","   - Store in a polars dataframe the sentiment for each case you selected.\n","   - Print:\n","     - The number of positive and negative predicted sentiments.\n","     - The explanation returned by the model for the first three cases of your sample.\n","3. Compare the 50 predictions with the true label on a confusion matrix."],"id":"52JhMeQ1m1gM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"lylvG2rUm1gN"},"outputs":[],"source":["# Sample 50 test reviews\n","\n","\n","\n","\n","\n","\n","\n","# Run the model and collect predictions\n","\n","\n","\n","\n","\n","\n","\n","# Convert results to Polars DataFrame\n","\n","\n","\n","\n","\n","\n","\n","# Print number of each predicted sentiment\n","\n","\n","\n","\n","\n","\n","\n","# Show explanations for the first 3 predictions\n","\n","\n","\n","\n","\n","\n","\n","# Generate and display confusion matrix\n","\n","\n","\n","\n","\n","\n"],"id":"lylvG2rUm1gN"},{"cell_type":"markdown","metadata":{"id":"UyZAtiYqm1gO"},"source":["\n","### Question 4: Reflect on the Model’s Reasoning\n","\n","Now that you've seen how the LLM classifies and explains sentiment, reflect on its behaviour in cases where it gets the label wrong.\n","1. Identify an example from Question 3 where the model's prediction disagreed with the ground truth label.\n","2. For the example:\n","   - Reread the review and the LLM’s explanation.\n","   - Try to understand why the model may have made a mistake.\n","   - Reflect on the following questions:\n","     - Was the review actually ambiguous or tricky?\n","     - Did the model misinterpret sarcasm, slang, or mixed sentiment?\n","     - Was the explanation reasonable, even if the label was wrong?"],"id":"UyZAtiYqm1gO"},{"cell_type":"markdown","metadata":{"id":"SpgPz0xxm1gO"},"source":["**Written Answer:**"],"id":"SpgPz0xxm1gO"},{"cell_type":"markdown","metadata":{"id":"JJ8MZbLSm1gO"},"source":["\n","### Question 5: Compare Prompt Engineering Strategies\n","\n","Prompt engineering plays a big role in how LLMs respond. In this question, you’ll try two different prompts and compare their effects on predictions and explanations.\n","1. Use Prompt A (simple) and Prompt B (instructional) below.\n","- Prompt A:  \n","  `Given the following movie review, classify it as Positive or Negative and explain why.`\n","- Prompt B:  \n","  `You are a helpful AI assistant trained in sentiment analysis. Your task is to determine whether a movie review is Positive or Negative, and clearly explain your reasoning.`\n","2. Apply both prompts to 3 reviews.\n","3. For each review:\n","   - Record the model's predicted sentiment and explanation for both prompts.\n","   - Note whether each prediction is correct or not.\n","4. Summarize your observations."],"id":"JJ8MZbLSm1gO"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UAlTMDBm1gO"},"outputs":[],"source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","    # Prompt A\n","\n","\n","\n","\n","\n","\n","\n","    # Prompt B\n","\n","\n","\n","\n","\n"],"id":"5UAlTMDBm1gO"}],"metadata":{"colab":{"provenance":[{"file_id":"1CZsqBj7LYgay8UbBDCSlGHs7uRi5-8Vs","timestamp":1745423776583}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":5}